# Super-transformers
This projects is mainly for coding llama style architecture LLMs but instead we use these three new ideas: 
- Scalabel sofmax
- Try to train it without the positional encodings
- super weights for pruning during inference
- 
